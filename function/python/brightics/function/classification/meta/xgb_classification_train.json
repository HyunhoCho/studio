{
    "script": {
        "type": "",
        "content": ""
    },
    "specJson": {
        "category": "classification",
        "func": "brightics.function.classification$xgb_classification_train",
        "name": "brightics.function.classification$xgb_classification_train",
        "context": "python",
        "label": "XGB Classification Train",
        "description": "\"XGBoost stands for 'Extreme Gradient Boosting', where the term 'Gradient Boosting' originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. This is a tutorial on gradient boosted trees, and most of the content is based on these slides by Tianqi Chen, the original author of XGBoost.\nThe gradient boosted trees has been around for a while, and there are a lot of materials on the topic. This tutorial will explain boosted trees in a self-contained and principled way using the elements of supervised learning. We think this explanation is cleaner, more formal, and motivates the model formulation used in XGBoost.\"",
        "tags": [
            "xgboost",
            "python",
            "tree",
            "classification",
            "train"
        ],
        "version": "3.6",
        "inputs": {
            "table": ""
        },
        "outputs": {
            "model": ""
        },
        "meta": {
            "table": {
                "type": "table"
            },
            "model": {
                "type": "model"
            }
        },
        "params": [
            {
                "id": "feature_cols",
                "label": "Feature Columns",
                "description": "Feature columns.",
                "mandatory": true,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "ColumnSelector",
                "columnType": [
                    "Integer",
                    "Long",
                    "Double",
                    "Float",
                    "Double[]"
                ],
                "validation": [],
                "multiple": true,
                "rowCount": 5
            },
            {
                "id": "label_col",
                "label": "Label Column",
                "description": "Label column.",
                "mandatory": true,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "ColumnSelector",
                "columnType": [
                    "Integer",
                    "Long",
                    "Double",
                    "Float",
                    "String"
                ],
                "validation": [],
                "multiple": false
            },
            {
                "id": "objective",
                "label": "Objective for Binary Classification",
                "description": "Specify the learning task and the corresponding learning objective or a custom objective function to be used. The objective options are below:\n\tbinary:logistic - logistic regression for binary classification, output probability.\n\tbinary:hinge - hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n\tcount:poisson - Poisson regression for count data, output mean of poisson distribution\n\tsurvival:cox - Cox regression for right censored survival time data (negative values are considered right censored)\n\trank:pairwise - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n\treg:gamma - gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed\n\treg:tweedie - Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.",
                "mandatory": false,
                "items": [
                    {
                        "label": "Logistic regression",
                        "value": "binary:logistic",
                        "default": true
                    },
                    {
                        "label": "Hinge loss for binary classification",
                        "value": "binary:hinge",
                        "default": false
                    },
                    {
                        "label": "Poisson regression for count data",
                        "value": "count:poisson",
                        "default": false
                    },
                    {
                        "label": "Cox regression for right censored survival time data",
                        "value": "survival:cox",
                        "default": false
                    },
                    {
                        "label": "Pairwise ranking",
                        "value": "rank:pairwise",
                        "default": false
                    },
                    {
                        "label": "Gamma regression with log-link",
                        "value": "reg:gamma",
                        "default": false
                    },
                    {
                        "label": "Tweedie regression with log-link",
                        "value": "reg:tweedie",
                        "default": false
                    }
                ],
                "visibleOption": [],
                "globalVariable": false,
                "control": "DropDownList",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": ""
            },
            {
                "id": "max_depth",
                "label": "Max Depth",
                "description": "Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in loss guided growing policy when tree_method is set as hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "InputBox",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": "3 (1 <= value)",
                "type": "Integer",
                "min": 1
            },
            {
                "id": "learning_rate",
                "label": "Learning Rate",
                "description": "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "InputBox",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": "0.1 (0 <= value)",
                "type": "Double",
                "min": 0
            },
            {
                "id": "n_estimators",
                "label": "Number of Trees",
                "description": "Number of gradient boosted trees. Equivalent to number of boosting rounds.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "InputBox",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": "100 (1 <= value)",
                "type": "Integer",
                "min": 1
            },
            {
                "id": "class_weight",
                "label": "Class Weights",
                "description": "Weights associated with classes. If weights are not given, all classes are supposed to have weight one. Note that the class labels are considered in lexicographical order.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "ArrayInput",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": "",
                "type": "Double"
            },
            {
                "id": "importance_type",
                "label": "Importance Type",
                "description": "The feature importance type for the feature_importances_ property: either gain, weight, cover, total_gain or total_cover",
                "mandatory": false,
                "items": [
                    {
                        "label": "Gain",
                        "value": "gain",
                        "default": true
                    },
                    {
                        "label": "Weight",
                        "value": "weight",
                        "default": false
                    },
                    {
                        "label": "Cover",
                        "value": "cover",
                        "default": false
                    },
                    {
                        "label": "Total Gain",
                        "value": "total_gain",
                        "default": false
                    },
                    {
                        "label": "Total Cover",
                        "value": "total_cover",
                        "default": false
                    }
                ],
                "visibleOption": [],
                "globalVariable": false,
                "control": "RadioButton",
                "columnType": [],
                "validation": [],
                "targetTable": []
            },
            {
                "id": "subsample",
                "label": "Fraction of Samples",
                "description": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. Fraction of Subsample interacts with the parameter Number of Trees. Choosing Fraction of Subsample < 1.0 leads to a reduction of variance and an increase in bias.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "InputBox",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "placeHolder": "1.0 (0 < value <= 1.0)",
                "type": "Double",
                "max": 1.0
            },
            {
                "id": "random_state",
                "label": "Seed",
                "description": "The seed used by the random number generator.",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "InputBox",
                "columnType": [],
                "validation": [],
                "targetTable": [],
                "type": "Integer"
            },
            {
                "id": "group_by",
                "label": "Group By",
                "description": "Columns to group by",
                "mandatory": false,
                "items": [],
                "visibleOption": [],
                "globalVariable": false,
                "control": "ColumnSelector",
                "columnType": [],
                "validation": [],
                "multiple": true,
                "rowCount": 5
            }
        ]
    },
    "md": ""
}